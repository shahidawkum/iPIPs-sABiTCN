{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "artF4WL6I31s"
      },
      "outputs": [],
      "source": [
        "!pip install torch esm pandas\n",
        "!pip install fair-esm --upgrade\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import esm\n",
        "# Load the ESM-2 model\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()  # Set the model to evaluation mode"
      ],
      "metadata": {
        "id": "NtSXQ_w5JAy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yR8GoV4LJF-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import esm\n",
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "\n",
        "# === Load ESM-2 (33-layer, 650M parameter model) ===\n",
        "print(\"ðŸ“¥ Loading ESM-2 650M (33 layers)...\")\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()  # evaluation mode\n",
        "\n",
        "# === Load sequences from FASTA ===\n",
        "fasta_file = \"/content/drive/MyDrive/Colab Notebooks/PIP-Training.fasta\"\n",
        "sequences = [(record.id, str(record.seq)) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
        "print(f\"âœ… Loaded {len(sequences)} sequences from FASTA\")\n",
        "\n",
        "# === Prepare input batches ===\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(sequences)\n",
        "\n",
        "# === Compute embeddings (Layer 33) ===\n",
        "print(\"ðŸ§  Extracting embeddings from layer 33...\")\n",
        "with torch.no_grad():\n",
        "    results = model(batch_tokens, repr_layers=[33])\n",
        "    token_representations = results[\"representations\"][33]\n",
        "\n",
        "# === Average token embeddings (excluding special tokens) ===\n",
        "sequence_representations = []\n",
        "for i, (_, seq) in enumerate(sequences):\n",
        "    rep = token_representations[i, 1:len(seq)+1].mean(0)\n",
        "    sequence_representations.append(rep)\n",
        "\n",
        "# === Convert to DataFrame and save ===\n",
        "sequence_representations = torch.stack(sequence_representations).numpy()\n",
        "df = pd.DataFrame(sequence_representations, index=[s[0] for s in sequences])\n",
        "output_file = \"ESM2_Layer33_PIPs.csv\"\n",
        "df.to_csv(output_file)\n",
        "print(f\"âœ… Features extracted from Layer 33 and saved as: {output_file}\")"
      ],
      "metadata": {
        "id": "hJearTmvJGVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}